{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * [Import Packages and Read credentials](#Import-Packages-and-Read-credentials)\n",
    "   * [Read Before Proceeding](#Permission-to-access-information-from-these-networking-sites-via-automated-means-to-be-obtained-before-running-scraping-codes-included-in-this-section)\n",
    "       * [Professional Networking site scraping example code](#Professional-Networking-Site)\n",
    "           * [Inner Functions](#Professional-Networking-Site-Scraping-INNER-FUNCTIONS)\n",
    "           * [Main Function (FUNCTION A)](#Professional-Networking-Site-Scraping-MAIN-FUNCTION)\n",
    "       * [Company Review site scraping example code](#Company-Review-Site)\n",
    "           * [Inner Functions](#Company-Review-Site-Scraping-INNER-FUNCTIONS)\n",
    "           * [Main Function (FUNCTION B)](#Company-Review-Site-Scraping-MAIN-FUNCTION)\n",
    "           \n",
    "           \n",
    "* [**START RUNNING HERE AFTER IMPORTING PACKAGES AND READING CREDENTIALS TO RUN CROSS REFERENCING CHECKS WITH SIMULATED DATA:** Example of Scraped data](#Example-of-scraped-data-to-input-as-data-for-testing-of-Company-background-online-presence-and-network-check)\n",
    "* [Company background online presence and network check](#Company-background-online-presence-and-network-check)\n",
    "    * [Inner Functions](#Company-background-online-presence-and-network-check-INNER-FUNCTIONS)\n",
    "    * [Main Functions (FUNCTION C)](#Company-background-online-presence-and-network-check-MAIN-FUNCTION)\n",
    "\n",
    "* [Scrape and Check Function](#Scrape-and-Check-Function)\n",
    "    - *Scrape and Check Function* a function incorporating the functionality of reading credentials and using driver within same directory to scraping the relevant webpages and processing the checks with FUNCTIONS A, B and C \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the possibility and logic to do a company background, online presence and network check in an effort to identify shell companies and prevent fraud transactions. Company information will be scraped from professional networking and company review sites and cross-referenced with google search results and some user-defined limit/minimal requirement (tweakable) to authenticate for entries to be genuine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages to be installed as instructed in README_CompanyCheck.txt and requirements_CompanyCheck.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install linkedin-scraper\n",
    "!pip install pattern\n",
    "!pip install bs4\n",
    "!pip install itertools\n",
    "!pip install textdistance\n",
    "!pip install string\n",
    "!pip install statistics\n",
    "!pip install os\n",
    "!pip install re\n",
    "!pip install pandas\n",
    "!pip install google-api-python-client\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages and Read credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from linkedin_scraper import actions\n",
    "from linkedin_scraper import Person\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import itertools\n",
    "import textdistance\n",
    "import string\n",
    "from statistics import mean \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import re\n",
    "from googleapiclient.discovery import build \n",
    "import pandas as pd\n",
    "from gensim.utils import lemmatize\n",
    "import en_core_web_sm\n",
    "#!python -m spacy download en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"cred.txt\", \"r\")\n",
    "cred = (f.read())\n",
    "email = cred.split(\"\\n\")[0]\n",
    "password = cred.split(\"\\n\")[1]\n",
    "ggl_api_key = cred.split(\"\\n\")[2]\n",
    "ggl_cse_id = cred.split(\"\\n\")[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permission to access information from these networking sites via automated means to be obtained before running scraping codes included in this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Professional Networking Site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Professional Networking Site Scraping INNER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeUrl_LinkedIn_about(company_name):\n",
    "   # company_name = 'Applied Materials'\n",
    "    replaced_company_name = company_name.replace(' ', \"-\").lower()\n",
    "    replaced_company_name\n",
    "\n",
    "    return(\"https://www.linkedin.com/company/\"+replaced_company_name+\"/about/\")\n",
    "\n",
    "makeUrl_LinkedIn_about('Applied Materials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeUrl_LinkedIn_jobs(company_name):\n",
    "   # company_name = 'Applied Materials'\n",
    "    replaced_company_name = company_name.replace(' ', \"-\").lower()\n",
    "    replaced_company_name\n",
    "\n",
    "    return(\"https://www.linkedin.com/company/\"+replaced_company_name+\"/jobs/\")\n",
    "\n",
    "makeUrl_LinkedIn_jobs('Applied Materials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDigit(string):\n",
    "    return int(re.findall(r'\\d+', string.replace(',', \"\").replace('', \"\"))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Professional Networking Site Scraping MAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: Linkedin email, linkedin password, company name\n",
    "#Output: Dictionary of information\n",
    "\n",
    "def linkedIn_name2dict(email,password,companyName):\n",
    "    try:\n",
    "        WINDOW_SIZE = \"1920,1080\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--window-size=%s\" % WINDOW_SIZE)\n",
    "\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        ### LOGIN ###\n",
    "        actions.login(driver, email, password) #package function\n",
    "\n",
    "        #INITIALISE STORAGE\n",
    "        comp_linkedin_Info = {}\n",
    "\n",
    "\n",
    "        ### ABOUT ###\n",
    "        #GO TO COMPANY URL\n",
    "        driver.get(makeUrl_LinkedIn_about(companyName))\n",
    "\n",
    "        #Extract information\n",
    "\n",
    "        #company linkedin employees\n",
    "        num_EmployeesC = driver.find_element_by_css_selector('div.mt1')\n",
    "        comp_linkedin_Info['Employees_on_Linkedin']  = int(re.findall(r'\\d+', num_EmployeesC.text.replace(',', \"\"))[0])\n",
    "\n",
    "        #the rest of details\n",
    "        labels = driver.find_elements_by_tag_name(\"dt\")\n",
    "        labels.insert(3, 'Subsidiaries')\n",
    "        values = driver.find_elements_by_tag_name(\"dd\")\n",
    "\n",
    "        numAttributes = min(len(labels), len(values))\n",
    "\n",
    "\n",
    "        for i in range(numAttributes):\n",
    "            try:\n",
    "                value = values[i].text\n",
    "            except:\n",
    "                value = values[i]\n",
    "\n",
    "            try:\n",
    "                label = labels[i].text\n",
    "            except:\n",
    "                label = labels[i]\n",
    "\n",
    "            #     print(label)\n",
    "            #     print(value)\n",
    "            #     print('')\n",
    "\n",
    "\n",
    "            if label == 'Subsidiaries':\n",
    "                comp_linkedin_Info[label] = value.split(\"subsidiaries: \")[1].split(\" and 1 more.\")[0].split(\",\")\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    comp_linkedin_Info[label] = extractDigit(value)\n",
    "                    #100001 is max number \n",
    "\n",
    "                except:\n",
    "                    comp_linkedin_Info[label] = value\n",
    "\n",
    "        ### JOBS ###\n",
    "\n",
    "        #GO TO COMPANY URL\n",
    "        driver.get(makeUrl_LinkedIn_jobs('Applied Materials'))\n",
    "\n",
    "\n",
    "        #Open Jobs\n",
    "        num_jobs = driver.find_element_by_css_selector('h4.org-jobs-job-search-form-module__headline').text\n",
    "        comp_linkedin_Info['Job Openings'] = extractDigit(num_jobs)\n",
    "        comp_linkedin_Info['Name'] = companyName\n",
    "\n",
    "        driver.quit()\n",
    "        return comp_linkedin_Info\n",
    "    \n",
    "    except:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        print('Try using another set of credentials or variation of company name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Function\n",
    "comp_linkedin_Info = linkedIn_name2dict(email,password,\"Applied Materials\")\n",
    "comp_linkedin_Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company Review Site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company Review Site Scraping INNER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeAchvDict(souped_splitted):\n",
    "\n",
    "    year = 0\n",
    "    achievement_dict = {}\n",
    "\n",
    "    for x in souped_splitted:\n",
    "\n",
    "        try :\n",
    "            (int(x))\n",
    "            new_year = x\n",
    "            try:\n",
    "                del achievement_dict[year][-1]\n",
    "            except:\n",
    "                pass\n",
    "            year = new_year\n",
    "            achievement_dict[year] = ['']\n",
    "        except:\n",
    "            #print('no')\n",
    "            if x[0]!=\",\":\n",
    "                achievement_dict[year][-1]+=(x.replace('\\xa0', \"\"))\n",
    "            else:\n",
    "                achievement_dict[year].append('')\n",
    "    del achievement_dict[year][-1]\n",
    "    return achievement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_thousand(string):\n",
    "    if string[-1]=='k':\n",
    "        return int(float(string.replace('k', \"\"))*1000)\n",
    "    else:\n",
    "        return int(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: title and overview in webelement, text and cons in texts\n",
    "#Output: List of lists. each inner list contains title, overview, pros and cons of 1 entry.\n",
    "\n",
    "def reviews_compiler_page(titles,overview,pros_cons):\n",
    "    company_reviews=[]\n",
    "    pros_extr,cons_extr = pros_cons_extractor(pros_cons)\n",
    "    for i in range(len(titles)):\n",
    "        company_reviews.append([titles[i].text,\n",
    "                               overview[i].text,\n",
    "                               pros_extr[i], \n",
    "                               cons_extr[i]])\n",
    "    return company_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: bag of ambiguous elements that may contain pros/cons/replies from companies\n",
    "#Output: return lists of pros and cons\n",
    "\n",
    "def pros_cons_extractor(pros_cons):\n",
    "    pros_list = []\n",
    "    cons_list = []\n",
    "    for i in range(len(pros_cons)):\n",
    "        relevant_text = pros_cons[i]\n",
    "        if \"Pros\\n\" in relevant_text.text:\n",
    "            pros_list.append(relevant_text.text.replace(\"Pros\\n\",\"\"))\n",
    "        elif \"Cons\\n\" in relevant_text.text:\n",
    "            cons_list.append(relevant_text.text.replace(\"Cons\\n\",\"\"))\n",
    "    return pros_list,cons_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Input a list with strings with leading and trailing spaces\n",
    "## Output: list with strings removed of leading and trailing spaces and lemmatized\n",
    "\n",
    "def lemmatize_remove_space(alist):\n",
    "    for i in range(len(alist)):\n",
    "        alist[i] = ' '.join([wd.decode('utf-8').split('/')[0] for wd in lemmatize(alist[i])])\n",
    "    return alist\n",
    "\n",
    "lemmatize_remove_space([' Bats Are ! hanging by their feet', ' qwe ', 'zxc '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Input a list with strings with captial letters and punctuations\n",
    "## Output: join all strings into 1es\n",
    "\n",
    "def proc_review(alist):\n",
    "    return ' '.join(lemmatize_remove_space(alist))\n",
    "\n",
    "\n",
    "proc_review(['bat be hang foot', 'qwe', 'zxc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: a list of fully preprocessed strings\n",
    "#Output: mean score of similarity between each combination of reviews\n",
    "def average_Sorensen_Dice_similarity(review_list_condensed):\n",
    "    similarity_score = []\n",
    "    for i in itertools.combinations(review_list_condensed, 2):\n",
    "        #https://itnext.io/string-similarity-the-basic-know-your-algorithms-guide-3de3d7346227\n",
    "        tokens_1 = i[0].split()\n",
    "        tokens_2 = i[1].split()\n",
    "        similarity_score.append(textdistance.sorensen(tokens_1 , tokens_2))\n",
    "    return mean(similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company Review Site Scraping MAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: company name\n",
    "#Output: Dictionary of information\n",
    "\n",
    "def glassdoor_name2dict(email,password,company_name_search_input):\n",
    "    try:\n",
    "\n",
    "        #driver = webdriver.Chrome()\n",
    "        #driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        WINDOW_SIZE = \"1920,1080\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--window-size=%s\" % WINDOW_SIZE)\n",
    "\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "\n",
    "        driver.get('https://www.glassdoor.sg/member/home/companies.htm')\n",
    "\n",
    "        #Initialise output\n",
    "        comp_glassdoor_Info={}\n",
    "\n",
    "\n",
    "        ### Log In ###\n",
    "        #Input cred\n",
    "        email_box = driver.find_element_by_id('userEmail')\n",
    "        pw_box = driver.find_element_by_id('userPassword')\n",
    "        email_box.send_keys(email)\n",
    "        pw_box.send_keys(password)\n",
    "\n",
    "\n",
    "\n",
    "        #click log in button\n",
    "        login_button = driver.find_element_by_css_selector('button.gd-ui-button.minWidthBtn.css-8i7bc2')\n",
    "        login_button.click()\n",
    "\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'sc.keyword')))\n",
    "\n",
    "        #Access SearchBox\n",
    "        s_box = driver.find_element_by_id('sc.keyword')\n",
    "        s_box.send_keys(company_name_search_input)\n",
    "\n",
    "        #Click on search\n",
    "        search_button = driver.find_element_by_css_selector('button.gd-ui-button.ml-std.col-auto.search__SearchStyles__newSearchButton.css-iixdfr')\n",
    "        search_button.click()\n",
    "        \n",
    "        #Get company website\n",
    "        comp_ws = driver.find_element_by_css_selector(\"p.webInfo.mb-0.mt-xxsm\").text\n",
    "        comp_glassdoor_Info['Website'] = comp_ws\n",
    "\n",
    "        #Access 1st result and create URL for company\n",
    "        boxed = driver.find_element_by_css_selector('div.col-3.logo-and-ratings-wrap')\n",
    "        messy = boxed.get_attribute('innerHTML')\n",
    "        partURL = messy.split('=\"')[1].split('\" ')[0]\n",
    "        fullURL = 'https://www.glassdoor.sg'+partURL\n",
    "\n",
    "        #To company URL\n",
    "        driver.get(fullURL)\n",
    "\n",
    "\n",
    "        ### Reached Company page ###\n",
    "        try:\n",
    "            #See more achievements\n",
    "            seeMoreAchv = driver.find_element_by_css_selector('button.d-flex.css-9k4uin.e18lin5w0')\n",
    "            seeMoreAchv.click()\n",
    "\n",
    "\n",
    "            #get all achievements\n",
    "            allAchvs = driver.find_element_by_class_name('css-1igu9t8.e19w1ug90')\n",
    "            achv_html = allAchvs.get_attribute('innerHTML')\n",
    "            achv_html_soup = BeautifulSoup(achv_html)\n",
    "            achv_souped_splitted = (achv_html_soup.get_text('\\n')).splitlines()\n",
    "            achvDict = makeAchvDict(achv_souped_splitted)\n",
    "\n",
    "            #Add achvdict to output dict\n",
    "            comp_glassdoor_Info['Awards & Accolades'] = achvDict\n",
    "        except:\n",
    "            comp_glassdoor_Info['Awards & Accolades'] = {}\n",
    "\n",
    "\n",
    "        #CEO info\n",
    "        ceo_box = driver.find_element_by_css_selector('div.d-lg-table-cell.ceoName.pt-sm.pt-lg-0.px-lg-sm')\n",
    "        comp_glassdoor_Info['CEO name']  = ceo_box.text.split('\\n')[0]\n",
    "        comp_glassdoor_Info['CEO ratings'] = ceo_box.text.split('\\n')[1]\n",
    "\n",
    "        #get number of reviews\n",
    "        reviews_box = driver.find_element_by_css_selector('a.eiCell.cell.reviews')\n",
    "        comp_glassdoor_Info['Company Reviews'] = int(make_thousand(reviews_box.text.split('\\n')[0]))\n",
    "\n",
    "        #get number of jobs\n",
    "        jobs_box = driver.find_element_by_css_selector('a.eiCell.cell.jobs')\n",
    "        comp_glassdoor_Info['Jobs'] = int(make_thousand(jobs_box.text.split('\\n')[0]))\n",
    "\n",
    "        #get number of salaries\n",
    "        sal_box = driver.find_element_by_css_selector('a.eiCell.cell.salaries')\n",
    "        comp_glassdoor_Info['Salaries'] = int(make_thousand(sal_box.text.split('\\n')[0]))\n",
    "\n",
    "        #get number of interview\n",
    "        interview_box = driver.find_element_by_css_selector('a.eiCell.cell.interviews')\n",
    "        comp_glassdoor_Info['Interview'] = int(make_thousand(interview_box.text.split('\\n')[0]))\n",
    "\n",
    "        #get number of benefit\n",
    "        benefit_box = driver.find_element_by_css_selector('a.eiCell.cell.benefits')\n",
    "        comp_glassdoor_Info['Benefit'] = int(make_thousand(benefit_box.text.split('\\n')[0]))\n",
    "\n",
    "        ### Go to reviews\n",
    "        #Access 1st result and create URL for review\n",
    "        review_box = driver.find_element_by_css_selector('a.eiCell.cell.reviews ')\n",
    "        review_url = review_box.get_attribute('href')\n",
    "\n",
    "        #To company URL\n",
    "        driver.get(review_url)\n",
    "\n",
    "        ############################Get reviews within page boundary of 5 (50 reviews)\n",
    "        user_page_limit=5\n",
    "        review_pages = []\n",
    "\n",
    "        next_page = True\n",
    "        while next_page:\n",
    "            titles =driver.find_elements_by_class_name('reviewLink')\n",
    "            overview = driver.find_elements_by_css_selector(\"p.mainText.mb-0\")\n",
    "            pros_cons = driver.find_elements_by_css_selector(\"div.v2__EIReviewDetailsV2__fullWidth \")\n",
    "\n",
    "            review_pages += reviews_compiler_page(titles,overview,pros_cons)\n",
    "\n",
    "            #Read current page and decide if go next page\n",
    "            reviewTotalInfo = driver.find_element_by_css_selector('div.paginationFooter').text.split(\" of \")\n",
    "            now_page = int(reviewTotalInfo[0].split(\" \")[-1])\n",
    "            limit_page = int(reviewTotalInfo[1].split(\" \")[0].replace(\",\",\"\"))\n",
    "            if (now_page < limit_page) and (now_page/10 <user_page_limit) :\n",
    "                #find next button and click\n",
    "                next_button = driver.find_element_by_class_name('nextButton.css-sed91k')\n",
    "                next_button.click()\n",
    "                try:\n",
    "                    WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, 'a.reviewLink')))\n",
    "                except:\n",
    "                    driver.get(driver.current_url)\n",
    "            else:\n",
    "                next_page = False\n",
    "        ############################\n",
    "\n",
    "        #create a list where an element is 1 review:\n",
    "        review_list_condensed = [proc_review(alist) for alist in review_pages]\n",
    "\n",
    "        #Add review similarity score to  output dict\n",
    "        comp_glassdoor_Info['Review similarity score'] = average_Sorensen_Dice_similarity(review_list_condensed)\n",
    "        comp_glassdoor_Info['Name'] = company_name_search_input\n",
    "        driver.quit()\n",
    "        return comp_glassdoor_Info\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        print('Try using another set of credentials or variation of company name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Function\n",
    "comp_glassdoor_Info = glassdoor_name2dict(email,password,\"Applied Materials\")\n",
    "comp_glassdoor_Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of scraped data to input as data for testing of Company background online presence and network check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_linkedin_Info = {'Employees_on_Linkedin': 21714,\n",
    " 'Website': 'http://www.appliedmaterials.com',\n",
    " 'Industry': 'Semiconductors',\n",
    " 'Company size': 10001,\n",
    " 'Subsidiaries': ['Applied Materials - Israel',\n",
    "  'Applied Materials India',\n",
    "  'Applied Materials South East Asia'],\n",
    " 'Headquarters': 'Santa Clara, CA',\n",
    " 'Type': 'Public Company',\n",
    " 'Specialties': 'Semiconductor Manufacturing Equipment, Materials Engineering, Flat Panel Display Manufacturing Equipment, and Global Services for the industries served',\n",
    " 'Job Openings': 1028,\n",
    "  'Name': 'Applied Materials'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_glassdoor_Info = {'Awards & Accolades': {'2015': ['Fortune 500,Fortune'],\n",
    "  '2014': [\"World's Most Admired Companies,Fortune Magazine\"],\n",
    "  '2013': [\"2013 World's Most Ethical Companies,Ethisphere Institute\",\n",
    "   'Preferred Quality Supplier (PQS) Award,Intel Corporation',\n",
    "   'Energy Innovator Award,Silicon Valley Power',\n",
    "   'Best Companies to Work For,Silicon India',\n",
    "   'Corporate Innovation Recognition Award,IEEE'],\n",
    "  '2012': ['Top Greenest Companies,NEWSWEEK',\n",
    "   'Top Military Friendly Employer,G.I. Jobs'],\n",
    "  '2011': ['Service Leadership Award,City Year San Jose/Silicon Valley',\n",
    "   'Technovation Semiconductor Ecosystem Award,India Semiconductor Association (ISA)',\n",
    "   '100 Best Places to Work in Information Technology,ComputerWorld',\n",
    "   'Top Corporate Philanthropist,Silicon Valley/San Jose Business Journal',\n",
    "   '100 Best Corporate Citizens,Corporate Responsibility Magazine'],\n",
    "  '2010': ['Supplier Excellence Award,Texas Instruments',\n",
    "   'EuroAsia IC Industry Subsystems & Components Award,EuroAsia Semiconductor Magazine',\n",
    "   'Best Places to Work for LGBT Equality,Human Rights Campaign'],\n",
    "  '2009': ['Top 100 Best Places to Work in IT,ComputerWorld',\n",
    "   \"Barron's 500,Barron's\",\n",
    "   'Top 500 Green Companies,Newsweek'],\n",
    "  '2008': ['World’s Most Admired Companies,Fortune',\n",
    "   'Training Top 125,Training']},\n",
    " 'CEO name': 'Gary Dickerson',\n",
    " 'CEO ratings': '573 Ratings',\n",
    " 'Company Reviews': 1900,\n",
    " 'Jobs': 803,\n",
    " 'Salaries': 7500,\n",
    " 'Interview': 473,\n",
    " 'Benefit': 531,\n",
    " 'Review similarity score': 0.31225144345500194,\n",
    " 'Name': 'Applied Materials',\n",
    "  'Website': 'http://www.appliedmaterials.com'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company background online presence and network check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company background, online presence and network check INNER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Input: glassdoor dictionary\n",
    "##Output: number of awards\n",
    "\n",
    "def num_awards(comp_glassdoor_Info):\n",
    "    award_dict = comp_glassdoor_Info['Awards & Accolades']\n",
    "    total_awards = 0\n",
    "    for i, v in award_dict.items():\n",
    "        total_awards+=len(v)\n",
    "    return total_awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2015 Fortune 500 Fortune',\n",
       "  \"2014 World's Most Admired Companies Fortune Magazine\",\n",
       "  \"2013 2013 World's Most Ethical Companies Ethisphere Institute\",\n",
       "  '2013 Preferred Quality Supplier (PQS) Award Intel Corporation',\n",
       "  '2013 Energy Innovator Award Silicon Valley Power',\n",
       "  '2013 Best Companies to Work For Silicon India',\n",
       "  '2013 Corporate Innovation Recognition Award IEEE',\n",
       "  '2012 Top Greenest Companies NEWSWEEK',\n",
       "  '2012 Top Military Friendly Employer G.I. Jobs',\n",
       "  '2011 Service Leadership Award City Year San Jose/Silicon Valley',\n",
       "  '2011 Technovation Semiconductor Ecosystem Award India Semiconductor Association (ISA)',\n",
       "  '2011 100 Best Places to Work in Information Technology ComputerWorld',\n",
       "  '2011 Top Corporate Philanthropist Silicon Valley/San Jose Business Journal',\n",
       "  '2011 100 Best Corporate Citizens Corporate Responsibility Magazine',\n",
       "  '2010 Supplier Excellence Award Texas Instruments',\n",
       "  '2010 EuroAsia IC Industry Subsystems & Components Award EuroAsia Semiconductor Magazine',\n",
       "  '2010 Best Places to Work for LGBT Equality Human Rights Campaign',\n",
       "  '2009 Top 100 Best Places to Work in IT ComputerWorld',\n",
       "  \"2009 Barron's 500 Barron's\",\n",
       "  '2009 Top 500 Green Companies Newsweek',\n",
       "  '2008 World’s Most Admired Companies Fortune',\n",
       "  '2008 Training Top 125 Training'],\n",
       " 'Applied Materials']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Input: glassdoor dictionary\n",
    "##Output: list of search items for awards\n",
    "\n",
    "def award_check_intermediate(comp_glassdoor_Info):\n",
    "    result = []\n",
    "    award_dict = comp_glassdoor_Info['Awards & Accolades']\n",
    "    total_awards = 0\n",
    "    for i, v in award_dict.items():\n",
    "        if len(v)>1:\n",
    "            for vv in v:\n",
    "                result.append((i + ' ' + vv).replace(\",\",\" \"))\n",
    "        else:\n",
    "            result.append((i + ' ' + v[0]).replace(\",\",\" \"))\n",
    "    return [result, comp_glassdoor_Info['Name']]\n",
    "\n",
    "\n",
    "#award_check_intermediate({'Awards & Accolades':{},'Name':'asd'})\n",
    "award_check_intermediate(comp_glassdoor_Info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Input: query, api key, cse key\n",
    "##Output: List of dictionaries, each dict a result \n",
    "\n",
    "def google_query(query, ggl_api_key, ggl_cse_id, **kwargs):\n",
    "    query_service = build(\"customsearch\", \n",
    "                          \"v1\", \n",
    "                          developerKey=ggl_api_key\n",
    "                          )  \n",
    "    query_results = query_service.cse().list(q=query,    # Query\n",
    "                                             cx=ggl_cse_id,  # CSE ID\n",
    "                                             **kwargs    \n",
    "                                             ).execute()\n",
    "    return query_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Input: company info in dictionary from scraping\n",
    "##Output: authentication from google searches\n",
    "\n",
    "def check_top10_google_results_allAwards(comp_glassdoor_Info):\n",
    "    awards, comp_name = award_check_intermediate(comp_glassdoor_Info)\n",
    "    \n",
    "    authenticity = False\n",
    "\n",
    "    for award in awards:\n",
    "        if not authenticity: #if non of the award has yet to be authenticated\n",
    "            this_Award_t10result = google_query(award+' '+comp_name,\n",
    "                          ggl_api_key, \n",
    "                          ggl_cse_id, \n",
    "                          num = 10\n",
    "                          ) #get top 10 result of this award\n",
    "\n",
    "            if int(this_Award_t10result['queries']['request'][0]['totalResults'])>0:\n",
    "                listo_results = this_Award_t10result['items']\n",
    "\n",
    "                for result in listo_results:\n",
    "                #a single search result on i-th award\n",
    "                    #check source\n",
    "                    source_link = result['pagemap']['metatags'][0]['og:url']\n",
    "                    source_siteCheck1 = 'glassdoor' not in  source_link# T/F returns if from glassdoor (the source we are trying to verify)\n",
    "                    source_siteCheck2 = 'linkedin' not in  source_link\n",
    "                    source_siteCheck = source_siteCheck1 & source_siteCheck2\n",
    "\n",
    "                    #check contents\n",
    "                    result_summary = (result['title'] + ' ' + result['snippet']).replace(\"\\n\", \" \") #header and contents\n",
    "                    #\n",
    "                    ##T/F if company is mentioned\n",
    "                    company_name_relevance = comp_name.lower() in result_summary.lower() \n",
    "                    #\n",
    "                    ##T/F if award is mentioned\n",
    "                    awardInfo = set(award.translate(str.maketrans('', '', string.punctuation)).lower().split(\" \"))\n",
    "                    searchInfo = set(result_summary.translate(str.maketrans('', '', string.punctuation)).lower().split(\" \"))\n",
    "                    #\n",
    "                    search_contentCheck = (awardInfo<=searchInfo)\n",
    "\n",
    "                    if search_contentCheck & source_siteCheck:\n",
    "                        authenticity = True\n",
    "                        break\n",
    "\n",
    "    return  authenticity    \n",
    "                \n",
    "#check_top10_google_results_allAwards({'Awards & Accolades':{},'Name':'asd'})                \n",
    "check_top10_google_results_allAwards(comp_glassdoor_Info)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Input: company info in dictionary from scraping and cut-off for minimal employees nodes on linkedin default at 100\n",
    "##Output: authentication from google searches in boolean\n",
    "\n",
    "def check_networkEmployees(comp_linkedin_Info, n1):\n",
    "    return (comp_linkedin_Info['Employees_on_Linkedin'] >=n1)\n",
    "\n",
    "    \n",
    "check_networkEmployees(comp_linkedin_Info, 100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Input: company info in dictionary from scraping and cut-off for minimal job openings on linkedin and glassdoor default at 20\n",
    "##Output: if job openings above cut-off in boolean\n",
    "\n",
    "def check_jobs(comp_linkedin_Info, comp_glassdoor_Info, n3):\n",
    "    TF = (comp_linkedin_Info['Job Openings'] >=n3) & (comp_glassdoor_Info['Jobs'] >=n3) \n",
    "    return TF\n",
    "\n",
    "check_jobs(comp_linkedin_Info,comp_glassdoor_Info, 20)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Input: company info in dictionary from scraping and limit for review similarity at default value of 50%\n",
    "##Output: company reviews are below limit in boolean\n",
    "\n",
    "def check_reviews(comp_glassdoor_Info, n2):\n",
    "    TF = (comp_glassdoor_Info['Review similarity score']<=n2)\n",
    "    return TF\n",
    "\n",
    "check_reviews(comp_glassdoor_Info, 0.5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Input: company info in dictionaries from scraping of diff sites\n",
    "##Output: if company official sites are the same in boolean\n",
    "\n",
    "def check_website(comp_glassdoor_Info, comp_linkedin_Info):\n",
    "    TF = (comp_glassdoor_Info['Website']==comp_linkedin_Info['Website'])\n",
    "    return TF\n",
    "\n",
    "check_website(comp_glassdoor_Info, comp_linkedin_Info)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Input: dictionary from scraping\n",
    "##Output: authentication CEO of company from google searches\n",
    "\n",
    "def check_CEO(comp_glassdoor_Info):\n",
    "    comp_name = award_check_intermediate(comp_glassdoor_Info)[1]\n",
    "    CEO_name = comp_glassdoor_Info['CEO name']\n",
    "    \n",
    "    authenticity = False\n",
    "\n",
    "\n",
    "    CEO_t10result = google_query('Who is the CEO of '+comp_name,\n",
    "                  ggl_api_key, \n",
    "                  ggl_cse_id, \n",
    "                  num = 10\n",
    "                  ) #get top 10 result of this award\n",
    "\n",
    "    if int(CEO_t10result['queries']['request'][0]['totalResults'])>0:\n",
    "        listo_results = CEO_t10result['items']\n",
    "\n",
    "        for result in listo_results:\n",
    "        #a single search result on i-th award\n",
    "            #check source\n",
    "            try:\n",
    "                source_link = result['pagemap']['metatags'][0]['og:url']\n",
    "                source_siteCheck1 = 'glassdoor' not in  source_link# T/F returns if from glassdoor (the source we are trying to verify)\n",
    "                source_siteCheck2 = 'linkedin' not in  source_link\n",
    "                source_siteCheck = source_siteCheck1 & source_siteCheck2\n",
    "                \n",
    "                #check contents\n",
    "                result_summary = (result['title'] + ' ' + result['snippet']).replace(\"\\n\", \" \") #header and contents\n",
    "\n",
    "                ##T/F if company is mentioned\n",
    "                company_name_relevance = comp_name.lower() in result_summary.lower() \n",
    "                #\n",
    "                ##T/F if CEO name is mentioned\n",
    "                CEO_Info = set(CEO_name.lower().split(\" \"))\n",
    "                searchInfo = set(result_summary.translate(str.maketrans('', '', string.punctuation)).lower().split(\" \"))\n",
    "                #\n",
    "                search_contentCheck = (CEO_Info<=searchInfo)\n",
    "\n",
    "\n",
    "                if search_contentCheck & source_siteCheck:\n",
    "                    authenticity = True\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return  authenticity    \n",
    "\n",
    "check_CEO(comp_glassdoor_Info)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company background online presence and network check MAIN FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to give level of credibility according to background check\n",
    "\n",
    "Possible output:\n",
    "- 'Fail to Authenticate'\n",
    "- 'Low'\n",
    "- 'Mid'\n",
    "- 'High'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'High'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Input: dictionaries from scraping\n",
    "##Output: Credibility classification\n",
    "\n",
    "def companyBackground_onlinePresenceAndNetwork_check(comp_linkedin_Info,comp_glassdoor_Info, n1,n2,n3):\n",
    "    #check if official company website is the same \n",
    "        ###check_website(comp_glassdoor_Info, comp_linkedin_Info)###\n",
    "        \n",
    "    #check review similiarity \n",
    "        ###check_reviews(comp_glassdoor_Info, cut_off =0.5)###\n",
    "    \n",
    "    #check job openings\n",
    "        ###check_jobs(comp_linkedin_Info,comp_glassdoor_Info, cut_off =20)###    \n",
    "    \n",
    "    #check linkedin employee base size\n",
    "        ###check_networkEmployees(comp_linkedin_Info, cut_off =100)###\n",
    "    \n",
    "    #check google search result for award recipient authentication\n",
    "        ###check_top10_google_results_allAwards(comp_glassdoor_Info)###\n",
    "        \n",
    "        \n",
    "    #check CEO from alternate source using google search\n",
    "        ###check_CEO(comp_glassdoor_Info)###\n",
    "            \n",
    "    #####################\n",
    "    \n",
    "    site_Check = check_website(comp_glassdoor_Info, comp_linkedin_Info)   \n",
    "    ceo_Check = check_CEO(comp_glassdoor_Info)  \n",
    "    \n",
    "    employeeNetwork_Check = check_networkEmployees(comp_linkedin_Info, n1)\n",
    "    awards_Check = check_top10_google_results_allAwards(comp_glassdoor_Info)\n",
    "    \n",
    "    reviews_Check = check_reviews(comp_glassdoor_Info, n2)\n",
    "    jobOpenings_Check = check_jobs(comp_linkedin_Info,comp_glassdoor_Info,n3)\n",
    "    \n",
    "    #####################\n",
    "    cred_failTOlow = site_Check & ceo_Check\n",
    "    \n",
    "    cred_lowTOmid = cred_failTOlow & employeeNetwork_Check & awards_Check\n",
    "    \n",
    "    cred_midTOhigh = cred_lowTOmid & reviews_Check & jobOpenings_Check\n",
    "    \n",
    "    credibility = 'Fail to Authenticate'\n",
    "    \n",
    "    if cred_midTOhigh:\n",
    "        credibility = 'High'\n",
    "    elif cred_lowTOmid:\n",
    "        credibility = 'Mid'\n",
    "    elif cred_failTOlow:\n",
    "        credibility = 'Low'\n",
    "    else:\n",
    "        credibility = 'Fail to Authenticate'\n",
    "    \n",
    "    return credibility\n",
    "\n",
    "companyBackground_onlinePresenceAndNetwork_check(comp_linkedin_Info,comp_glassdoor_Info, n1 = 100, n2 = 0.5, n3=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape and Check Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function runs all the functions aforementioned with \n",
    "- Input: Company name\n",
    "- Output: Credibility classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cred_check(company_name, n1 = 100, n2 = 0.5, n3=20):\n",
    "    \n",
    "    #credentials\n",
    "    f = open(\"cred.txt\", \"r\")\n",
    "    cred = (f.read())\n",
    "    email = cred.split(\"\\n\")[0]\n",
    "    password = cred.split(\"\\n\")[1]\n",
    "    ggl_api_key = cred.split(\"\\n\")[2]\n",
    "    ggl_cse_id = cred.split(\"\\n\")[3]\n",
    "    \n",
    "    #company info\n",
    "    comp_linkedin_Info = linkedIn_name2dict(email,password,company_name)\n",
    "    comp_glassdoor_Info = glassdoor_name2dict(email,password,company_name)\n",
    "    \n",
    "    #check\n",
    "    return companyBackground_onlinePresenceAndNetwork_check(comp_linkedin_Info,comp_glassdoor_Info,n1,n2,n3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred_check('Applied Materials')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
